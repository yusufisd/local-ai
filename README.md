# Local AI - Ollama Entegrasyonlu Chat UygulamasÄ±

Bu proje, lokal olarak Ã§alÄ±ÅŸan Ollama AI modellerini kullanan bir Next.js chat uygulamasÄ±dÄ±r.

## Ã–zellikler

- ğŸ¤– Ollama ile lokal AI desteÄŸi
- ğŸ’¬ Modern chat arayÃ¼zÃ¼
- ğŸ“± Responsive tasarÄ±m
- ğŸ¨ shadcn/ui componentleri

## Kurulum

### 1. BaÄŸÄ±mlÄ±lÄ±klarÄ± YÃ¼kleyin

```bash
npm install
# veya
yarn install
# veya
pnpm install
```

### 2. Ollama'yÄ± Kurun ve BaÅŸlatÄ±n

[Ollama'yÄ± resmi web sitesinden](https://ollama.org/) indirip kurun. Kurulumdan sonra Ollama'yÄ± baÅŸlatÄ±n:

```bash
# Ollama'nÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± kontrol edin
curl http://localhost:11434/api/tags
```

### 3. Bir Model Ä°ndirin

Kullanmak istediÄŸiniz bir modeli indirin (Ã¶rnekler):

```bash
ollama pull llama3.2
# veya
ollama pull mistral
# veya
ollama pull codellama
```

Mevcut modelleri gÃ¶rmek iÃ§in:
```bash
ollama list
```

### 4. Ortam DeÄŸiÅŸkenlerini AyarlayÄ±n

`.env.local` dosyasÄ± oluÅŸturun (`.env.local.example` dosyasÄ±nÄ± referans alarak):

```bash
cp .env.local.example .env.local
```

`.env.local` dosyasÄ±nÄ± dÃ¼zenleyin:

```env
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2  # Ä°ndirdiÄŸiniz model adÄ±nÄ± buraya yazÄ±n
```

### 5. GeliÅŸtirme Sunucusunu BaÅŸlatÄ±n

```bash
npm run dev
# veya
yarn dev
# veya
pnpm dev
```

TarayÄ±cÄ±nÄ±zda [http://localhost:3000](http://localhost:3000) adresini aÃ§Ä±n.

## KullanÄ±m

1. Ollama'nÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun (`http://localhost:11434`)
2. UygulamayÄ± aÃ§Ä±n ve sohbet etmeye baÅŸlayÄ±n!
3. Sidebar'dan sohbet geÃ§miÅŸinize ve ayarlara eriÅŸebilirsiniz

## YapÄ±landÄ±rma

### FarklÄ± Bir Model Kullanma

`.env.local` dosyasÄ±ndaki `OLLAMA_MODEL` deÄŸerini deÄŸiÅŸtirin:

```env
OLLAMA_MODEL=mistral
```

### FarklÄ± Bir Ollama URL'i

EÄŸer Ollama farklÄ± bir portta Ã§alÄ±ÅŸÄ±yorsa:

```env
OLLAMA_BASE_URL=http://localhost:11435
```

## Sorun Giderme

### Ollama'ya BaÄŸlanÄ±lamÄ±yor

- Ollama'nÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun: `curl http://localhost:11434/api/tags`
- `.env.local` dosyasÄ±ndaki `OLLAMA_BASE_URL` deÄŸerini kontrol edin
- Firewall ayarlarÄ±nÄ± kontrol edin

### Model BulunamadÄ±

- Modelin indirildiÄŸinden emin olun: `ollama list`
- `.env.local` dosyasÄ±ndaki `OLLAMA_MODEL` adÄ±nÄ±n doÄŸru olduÄŸunu kontrol edin

## Teknolojiler

- [Next.js](https://nextjs.org/)
- [React](https://react.dev/)
- [TypeScript](https://www.typescriptlang.org/)
- [Tailwind CSS](https://tailwindcss.com/)
- [shadcn/ui](https://ui.shadcn.com/)
- [Ollama](https://ollama.org/)
